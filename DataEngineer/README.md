# Data Science Engineering Exercise
### Setting
At BluNova you will be working with live data sources which run and evolve independent to us. We are looking for someone with a keen eye for detail who will assist with building robust pipelines that can handle changes over time. We treat ETL as a continuous evolving process and acknowledge that there will always be new surprises around every corner.

Though the focus of this assessment is on showing a strong understanding of ETL as it is a core function, the scope of the role is much broader and includes significant room to learn, grow and challenge oneâ€™s self. As an example, our core tools make extensive use of Spark as well as Databricks managed services, AWS cloud infrastructure, Terraform and extending into the rest of the Hashicorp stack, Python, and a full decisioning environment that is currently being selected to be implemented in the next few months. 

Our primary requirements from a candidate are to demonstrate a strong functional knowledge of ETL and its best practices, with experience of managing data at scale a bonus. Resilience to find a way to solve difficult problems and an aptitude for learning new and sometimes challenging things to support our future needs. Being both self-motivated and valuing collaboration over individual work. Cloud experience is preferential, but not a prerequisite. Motivate team members and demonstrate behaviour in line with the core values and team ethos which include responsibility, empathy, self-awareness, optimism, curiosity, integrity and kindness. We are looking for a candidate who will thrive in an environment where the core work ethic incudes enjoying having the freedom to direct processes based on their own insight, asking why and not just doing it the way it's always been done, taking responsibility for projects they work on, and putting time aside for aggressive automation of repetitive tasks with strong tests and documentation in place.    
   
### Instructions
1. Suppose you are given the clean set and told to build an ETL pipeline.
2. A few weeks later a data scientist comes to you in a panic stating all the data is wrong. Looking at the dirty set to identify what went wrong and suggest some remedial measures to refactor your data pipeline to be robust. This is an open ended question so look at the data and imagine some scenario of your own as well.
3. Rather than coding up the solution please provide a document with your findings.

We want to see that you can find, understand, fix and communicate the problems, so clear and thorough explanation of problems and solutions is important.

Good luck!
